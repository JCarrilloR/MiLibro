# Analisis metatranscriptomicos (RNA-seq)

Descripción del capítulo

## Descragando secuencias crudas de un bioproject. Para recuperar las secuencias crudas asociadas al BioProject PRJNA935796, se utilizó el paquete EDirect en combinación con el SRA Toolkit. En primer lugar, se consultó la base de datos SRA mediante el comando esearch, recuperando la tabla de metadatos en formato runinfo con efetch, la cual fue almacenada en un archivo (runinfo.csv). A partir de este archivo se extrajo la columna Run, que contiene los identificadores únicos de cada corrida de secuenciación (SRR), generando así una lista (SRR_list.txt). Finalmente, cada uno de los accesos SRR fue descargado de manera iterativa mediante el comando fasterq-dump, utilizando ocho hilos en paralelo (-e 8) y guardando las lecturas crudas en la carpeta raw_sequences.

# Obtener lista de SRR
esearch -db sra -query PRJNA935796 | efetch -format runinfo > runinfo.csv

# Extraer la columna "Run" (SRR IDs)
cut -d',' -f1 runinfo.csv | grep SRR > SRR_list.txt

# Descargar todas las muestras
for srr in $(cat SRR_list.txt); do
  fasterq-dump $srr -O ./raw_sequences -e 12
done




## Analisis de calidad inicial. Se realizó un análisis de control de calidad con FastQC sobre todas las lecturas crudas en formato FASTQ, considerando archivos comprimidos y no comprimidos correspondientes a lecturas forward y reverse. Los reportes se generaron en paralelo utilizando ocho hilos de procesamiento y se almacenaron en la carpeta fastqc_raw, que contiene tanto los archivos HTML para la inspección visual de la calidad como los archivos comprimidos con los datos numéricos de cada muestra.

gunzip *.gz

#!/bin/zsh

set -e                    # Detener el script si ocurre un error
setopt NULL_GLOB          # Si no hay coincidencias, los patrones se expanden a vacío

outdir="fastqc_raw"       # Carpeta de salida para los resultados
mkdir -p "$outdir"        # Crear la carpeta si no existe

# Buscar archivos FASTQ crudos (acepta .fastq y .fastq.gz)
typeset -a RAW_FILES
RAW_FILES=( *_R1_001.fastq *_R2_001.fastq *_R1_001.fastq.gz *_R2_001.fastq.gz )

# Verificar que existan archivos
(( ${#RAW_FILES} > 0 )) || { echo "No se encontraron archivos FASTQ crudos en el directorio actual."; exit 1; }

# Ejecutar FastQC con 8 hilos sobre todos los archivos encontrados
fastqc -t 8 -o "$outdir" $RAW_FILES

echo "✅ Análisis de FastQC completado. Resultados guardados en: $PWD/$outdir"


##Se integraron los reportes individuales de FastQC correspondientes a las lecturas crudas mediante MultiQC, generando un informe consolidado en formato HTML con las principales métricas de calidad. Asimismo, el archivo tabular multiqc_general_stats.txt, que resume los valores numéricos de todas las muestras, fue copiado al directorio raíz del proyecto para facilitar su comparación y análisis posterior.

#!/bin/zsh

set -euo pipefail
# -e: salir si ocurre error
# -u: error si se usa variable no definida
# -o pipefail: propaga errores en pipelines

indir="fastqc_raw"       # Carpeta donde están los resultados de FastQC
outdir="fastqc_raw/multiqc_data"  # Carpeta donde MultiQC guardará resultados
mkdir -p "$indir"        # Crea la carpeta de entrada si no existe (por seguridad)

# Verifica que existan reportes de FastQC
ls "$indir"/*fastqc.zip >/dev/null 2>&1 || { echo "⚠️ No se encontraron resultados de FastQC en $indir"; exit 1; }

# Ejecuta MultiQC sobre fastqc_raw
multiqc "$indir" --outdir "$indir" --filename multiqc_fastqc_raw

# Copia el archivo de estadísticas generales al directorio raíz
cp fastqc_raw/multiqc_fastqc_raw_data/multiqc_general_stats.txt ./multiqc_general_stats.txt


## Limpieza de secuencias
# Raw paired-end RNA-seq reads were processed with Cutadapt (vX.X) to remove adapter contamination and low-quality bases. TruSeq Illumina adapter sequences were trimmed from both read ends, and poly-A tails of ≥10 nucleotides were clipped when present. Quality trimming was applied with a Phred threshold of 20 at both 5′ and 3′ ends, and reads shorter than 20 nucleotides after trimming were discarded. Filtering also removed reads containing ambiguous bases (N). Processing was performed in parallel across all samples, and per-sample reports were generated and subsequently summarized with MultiQC.

## Las lecturas crudas pareadas de RNA-seq se procesaron con Cutadapt (vX.X) para eliminar la contaminación por adaptadores y las bases de baja calidad. Las secuencias de adaptadores TruSeq de Illumina se recortaron en ambos extremos de las lecturas y, cuando estuvieron presentes, se eliminaron colas poli-A de ≥10 nucleótidos. Se aplicó un recorte por calidad con un umbral Phred de 20 en los extremos 5′ y 3′, y las lecturas con una longitud final menor a 20 nucleótidos se descartaron. Asimismo, se filtraron las lecturas que contenían bases ambiguas (N). El procesamiento se realizó en paralelo para todas las muestras y se generaron reportes individuales, que posteriormente fueron resumidos con MultiQC.

```
#!/bin/zsh

set -e                    # Salir si algún comando falla
setopt NULL_GLOB         # En zsh: patrones sin match se expanden a “nada” (no quedan literales)

outdir="cutadapt_results"
mkdir -p "$outdir"

# Construye la lista de archivos R1 válidos (acepta .fastq y .fastq.gz)
typeset -a R1_FILES
R1_FILES=( *_R1_001.fastq *_R1_001.fastq.gz )

# Si no hay archivos que coincidan, termina con mensaje claro
if (( ${#R1_FILES} == 0 )); then
  echo "No se encontraron archivos que coincidan con *_R1_001.fastq(.gz) en el directorio actual."
  exit 1
fi

# Recorre cada R1 y empareja su R2 correspondiente
for R1 in $R1_FILES; do
  if [[ "$R1" == *.fastq.gz ]]; then
    base="${R1%_R1_001.fastq.gz}"          # nombre sin sufijo
    R2="${base}_R2_001.fastq.gz"           # par esperado
    ext=".fastq.gz"                        # extensión de salida
  else
    base="${R1%_R1_001.fastq}"
    R2="${base}_R2_001.fastq"
    ext=".fastq"
  fi

  # Verifica que exista el par R2
  if [[ ! -f "$R2" ]]; then
    echo "⚠️  No se encontró el par R2 para: $R1   (esperado: $R2). Se omite."
    continue
  fi

  echo "Procesando: R1=$R1   R2=$R2"

  # Ejecuta cutadapt en UNA sola línea (evita problemas al pegar en zsh)
  cutadapt -j 8 -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA -A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT -a A{10} -A A{10} -q 30,30 -m 20 --max-n 0 --overlap 5 -e 0.1 --trim-n --pair-filter=any --json "${outdir}/${base:t}.cutadapt.json" --report=minimal -o "${outdir}/${base:t}_R1.trimmed${ext}" -p "${outdir}/${base:t}_R2.trimmed${ext}" "$R1" "$R2"
done

echo "Análisis completado. Resultados guardados en: ${outdir}/"


##Notas (línea por línea)
##!/bin/zsh → fuerza a ejecutar con zsh (aunque tu shell por defecto sea otro).
##set -e → si algo falla, el script se detiene (evitas resultados a medias).
##setopt NULL_GLOB → en zsh, si el patrón no coincide, se expande a vacío (no deja literales como *_R1_001.fastq).
##outdir="cutadapt_results" y mkdir -p "$outdir" → define y crea la carpeta de salida.
##R1_FILES=( *_R1_001.fastq *_R1_001.fastq.gz ) → arma la lista de archivos R1 válidos, aceptando ambos formatos.
##if (( ${#R1_FILES} == 0 )), fi es simplemente el cierre de una estructura if→ si la lista está vacía, termina con un mensaje claro.
##Bloque if [[ "$R1" == *.fastq.gz ]] → detecta si la muestra es comprimida o no y ajusta sufijos/extensiones.
##[[ ! -f "$R2" ]] → valida que el par R2 exista; si no, lo salta con aviso.
##Comando cutadapt en una sola línea → evita que zsh interprete líneas sueltas como comandos (-a, -A, etc.).
##"${base:t}" → en zsh, :t es el basename (quita ruta), por si ejecutas desde otra carpeta.
##Mensaje final con la ruta de resultados.
##El comando de Cutadapt se empleó para el recorte de adaptadores y la depuración de lecturas pareadas en formato FASTQ. Se ejecutó con 8 hilos en paralelo (-j 8) especificando las secuencias de adaptadores para las lecturas forward y reverse (-a y -A), así como la eliminación de colas poli-A de al menos diez nucleótidos. Se aplicó un filtrado de calidad en ambos extremos de las lecturas (-q 20,20), estableciendo una longitud mínima de 20 pb (-m 20), con un máximo de 0 nucleótidos ambiguos permitidos (--max-n 0). Para la detección de adaptadores, se definió un solapamiento mínimo de 5 pb (--overlap 5) y una tasa máxima de error de 0.1 (-e 0.1). Se activó además la opción de recortar nucleótidos ambiguos en los extremos (--trim-n) y de descartar un par si alguna de las lecturas no cumplía los criterios (--pair-filter=any). Los resultados se almacenaron en archivos FASTQ recortados para cada par de lecturas, junto con un reporte resumido en formato JSON que describe las estadísticas del proceso.


## Después del recorte de secuencias con Cutadapt, las lecturas obtenidas fueron evaluadas en términos de calidad mediante un flujo automatizado en zsh. El procedimiento consistió en localizar los archivos recortados en la carpeta cutadapt_results, sobre los cuales se ejecutó FastQC para generar reportes individuales con métricas como calidad por base, contenido GC, duplicación y longitudes de lectura. Posteriormente, los resultados de FastQC se integraron mediante MultiQC, obteniéndose un reporte consolidado en formato HTML y archivos tabulares que permiten la comparación global de todas las muestras en un solo documento.

#!/bin/zsh

set -e
# Sale inmediatamente si algún comando falla (evita resultados a medias)

setopt NULL_GLOB
# En zsh, hace que los patrones que no coinciden no queden literales (se expanden a vacío)

outdir="cutadapt_results"
# Nombre de la carpeta donde quedaron los archivos recortados por cutadapt

[[ -d "$outdir" ]] || { echo "No existe la carpeta '$outdir'. Corre primero el recorte con cutadapt."; exit 1; }
# Verifica que exista la carpeta de resultados de cutadapt (si no, termina con mensaje claro)

cd "$outdir"
# Entra a la carpeta de resultados de cutadapt

typeset -a TRIMMED
TRIMMED=( *.trimmed.fastq *.trimmed.fastq.gz )
# Construye la lista de archivos recortados a evaluar con FastQC (acepta .fastq y .fastq.gz)

(( ${#TRIMMED} > 0 )) || { echo "No se encontraron archivos *.trimmed.fastq(.gz) en '$outdir'."; exit 1; }
# Si no hay archivos recortados, termina con mensaje claro

mkdir -p fastqc_results
# Crea una subcarpeta para almacenar los reportes de FastQC

fastqc -t 8 -o fastqc_results $TRIMMED
# Ejecuta FastQC con 8 hilos, guardando todos los reportes dentro de fastqc_results

cd fastqc_results
# Entra a la subcarpeta con los reportes de FastQC

multiqc . --outdir . --filename multiqc_trimmed_fastqc
# Ejecuta MultiQC sobre los outputs de FastQC en esta carpeta; genera multiqc_trimmed_fastqc.html y multiqc_data/

echo "Análisis completado. Revisa: $(pwd)/multiqc_trimmed_fastqc.html"
# Mensaje final con la ruta completa del reporte consolidado


## El procedimiento consistió en la integración de los reportes de MultiQC obtenidos antes (raw) y después (trimmed) del recorte de secuencias. Para ello, los archivos multiqc_general_stats.txt fueron procesados de manera que cada muestra quedara normalizada, eliminando sufijos técnicos como _001, _R1 o _R2. A cada registro se le añadió una columna denominada Tipo, que permitió distinguir entre datos crudos y recortados. Finalmente, ambos conjuntos se unificaron en un solo archivo tabular, lo que posibilita la comparación directa de las métricas de calidad de secuenciación entre etapas del flujo de análisis.


#!/bin/zsh

set -euo pipefail
# -e: salir si algo falla; -u: error si usas variables no definidas; -o pipefail: propaga errores en pipes

# Rutas de entrada (ajusta si tus nombres difieren)
F_TRIM="cutadapt_results/fastqc_results/multiqc_trimmed_fastqc_data/multiqc_general_stats.txt"   # resumen de MultiQC post-trimming
F_RAW="fastqc_raw/multiqc_general_stats.txt"                                                     # resumen de MultiQC de datos crudos
OUT="comparativo_general_stats.txt"    

[[ -f "$F_TRIM" ]] || { echo "No se encuentra: $F_TRIM"; exit 1; }
[[ -f "$F_RAW"  ]] || { echo "No se encuentra: $F_RAW";  exit 1; }

# Función awk para procesar cada archivo en formato largo
process_file() {
  local file=$1
  local tipo=$2
  awk -v TIPO="$tipo" -F'\t' 'NR==1 {
      # Encabezado: solo se imprime una vez
      if (!printed++) {
        printf "Sample\tTipo"
        for (i=2;i<=NF;i++) printf "\t%s", $i
        printf "\n"
      }
      next
    }
    {
      # Normaliza nombre de muestra:
      sample=$1
      sub(/_001$/, "", sample)     # quita sufijo _001
      sub(/_R[12]$/, "", sample)   # opcional: elimina _R1/_R2 si quieres agrupar sin distinción de par
      printf "%s\t%s", sample, TIPO
      for (i=2;i<=NF;i++) printf "\t%s", $i
      printf "\n"
    }' "$file"
}

# Procesa ambos archivos y combina
process_file "$F_TRIM" "trimmed" > "$OUT"
process_file "$F_RAW"  "raw"     >> "$OUT"

echo "✅ Archivo unificado generado: $PWD/$OUT"


##Las lecturas filtradas fueron procesadas con SortMeRNA para la identificación y eliminación de secuencias de ARN ribosomal (rRNA). El análisis se realizó utilizando las bases de datos de referencia provistas por la herramienta (SILVA y Rfam), clasificando las lecturas en dos conjuntos: rRNA y no rRNA. Las lecturas no ribosomales se conservaron para los análisis posteriores de expresión y anotación funcional, con el fin de reducir la sobre-representación de rRNA en las bibliotecas y mejorar la calidad de la información transcriptómica.

##Trimmed reads were processed with SortMeRNA to identify and remove ribosomal RNA (rRNA) sequences. The analysis was performed using the reference databases provided by the software (SILVA and Rfam), classifying reads into two groups: rRNA and non-rRNA. Non-ribosomal reads were retained for downstream expression and functional annotation analyses, in order to reduce rRNA overrepresentation in the libraries and improve transcriptomic data quality.


###En mi caso se instalo micromamba como ambiente y se instalo SortMeRNA


### Subtítulo nivel 2

Contenido
